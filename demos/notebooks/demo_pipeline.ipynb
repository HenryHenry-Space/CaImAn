{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be sure to clear all outputs before pushing\n",
    "# !git branch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNMF demo pipeline: Intro\n",
    "This demo presents a full pipeline for the analysis of a two-photon calcium imaging dataset using the CaImAn (**Ca**lcium **Im**aging **An**alysis) software package. Starting with loading the original movie, it demonstrates how to use Caiman's built-in tools for the following analysis steps:\n",
    "\n",
    "![temporary workflow image](../../docs/img/quickintro.png)\n",
    "\n",
    "- Using the NoRMCorre (nonrigid motion correction) algorithm for motion correction.\n",
    "- Using constrained nonnegative matrix factorization (CNMF) algorithm to extract an initial estimate of the neurons' location, calcium traces, and firing rates.  \n",
    "- Use quality control metrics to evaluate the initial estimates, and narrow down to the final set of estimates.\n",
    "- Extract normalized traces $\\Delta F/F$.\n",
    "\n",
    "The CNMF algorithm is best for data with relatively low background noise, like most two-photon data and *some* one photon data like certain light sheet data. For a demo analysis pipeline of a one-photon microendoscopic data set see `demo_pipeline_cnmfE.ipynb`.\n",
    "\n",
    "The dataset used in this demo was provided courtesy of Sue Ann Koay and David Tank (Princeton University). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h2 style=\"margin-top: 0;\">Getting more help</h2>\n",
    "    More detailed background information about CNMF can be found in the <a href=\"https://pubmed.ncbi.nlm.nih.gov/26774160/\">original CNMF paper</a> and <a href=\"https://pubmed.ncbi.nlm.nih.gov/30652683/\">the Caiman paper</a>. If you have specific questions about this demo, or the underlying algorithms, you can ask questions at our <a href=\"https://app.gitter.im/#/room/#agiovann_Constrained_NMF:gitter.im\">Gitter channel</a>. If you find a bug or you have a feature request, feel free to <a href=\"https://github.com/flatironinstitute/CaImAn/issues\">open an issue at our Github repo</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do: \n",
    "- Add table of contents with different sections\n",
    "- Add \"how to read this\" (don't just run it all at once, it is mean tot be read, some sections are ment to be run iteratively as you modify parameters). If you have to explain how to read this, you probably are doing somethign wrong. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and general setup\n",
    "We first need to import the Python libraries we will use in the rest of the notebook and tweak some general settings. Don't worry about these details now, we will explain the important things when they come up. Note in the following, we import `caiman` as `cm`, so when you see `cm` in the rest of the notebook, it just means you are using something from the Caiman package.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh.plotting as bpl\n",
    "import cv2\n",
    "import glob\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import psutil\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    cv2.setNumThreads(0)\n",
    "except():\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    if __IPYTHON__:\n",
    "        # reloads modules automatically when they are changed\n",
    "        ipython().magic('load_ext autoreload')\n",
    "        ipython().magic('autoreload 2')\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "import caiman as cm\n",
    "from caiman.motion_correction import MotionCorrect\n",
    "from caiman.source_extraction.cnmf import cnmf, params\n",
    "from caiman.utils.utils import download_demo\n",
    "from caiman.utils.visualization import plot_contours, nb_view_patches, nb_plot_contour\n",
    "\n",
    "bpl.output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing with our basic setup, we will set up a logger, and also set some environment variables in case that wasn't done already in your shell. If you want to learn more about Caiman's logger functionality, or tweak your logger, see [Appendix 1: Logging](#logging_explained). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "logging.basicConfig(format=\"{asctime} - {levelname} - [{filename} {funcName}() {lineno}] - pid {process} - {message}\",\n",
    "                    filename=None, \n",
    "                    level=logging.WARNING, style=\"{\") #logging level can be DEBUG, INFO, WARNING, ERROR, CRITICAL\n",
    "\n",
    "# set env variables \n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify data to be processed\n",
    "For this demo, we are going to analyze the data file `Sue_2x_3000_40_-46.tif`. This 3000-frame movie, provided by Sue Koay and David Tank, is two-photon data from supragranular parietal cortex of a GCaMP6f-expressing mouse during a virtual reality task. It was collected at 30Hz, and to save space, the demo data has been spatially cropped and downsampled by a factor of 2 compared to the original. \n",
    "\n",
    "To get the data, we will use Caiman's built-in `download_demo()` function. It will download the data to  `~/caiman_data/example_movies/` where `~` is your home directory (the path format and home directory will depend on your operating system). If you already have the movie, it will just return the path to the movie. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_path = download_demo('Sue_2x_3000_40_-46.tif')\n",
    "print(f\"Original movie for demo is in {movie_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to adapt this demo for your own data, just direct the `movie_path` variable to your own movie:\n",
    "\n",
    "    movie_path = 'full/path/to/your/movie.filetype'\n",
    "\n",
    "If you have a recording system that breaks up the data across multiple files, see [Appendix 2: Working with multiple files](#multiple_files). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h2 style=\"margin-top: 0;\">File types that Caiman can read</h2>\n",
    "    While this demo uses a movie that has been stored in <i>tif</i> format, Caiman can handle movies in multiple common (and not so common) formats, including:\n",
    "\n",
    "    hdf5/h5, n5, zarr, avi, nwb, npz\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and visualize raw data\n",
    "Caiman has a built-in movie class that you can use to view your movie. \n",
    "\n",
    "The movie object has many convenient features. Once you have loaded the movie (using `cm.load()`), you can view your raw data using `movie.play()`. This `play()` function has multiple parameters you can explore, including: \n",
    "\n",
    "    gain: brightness \n",
    "    fr:  frame rate\n",
    "    magnification: scale the size of the display  \n",
    "    qmax, q_min: percentile for setting vmax, vmin -- below vmin is black, above vmax is white\n",
    "    plot_text (Bool): show the frame number\n",
    "    \n",
    "The movie object also has a `resize()` method, which we use in the following to downsample the movie by 5x before playing using the `downsampling_ratio`. \n",
    "\n",
    "Playing the movie uses the OpenCV library, so if you set `display_movie` to `True`, the following cell will run a blocking function (a function that blocks execution of all other code until it is stopped), opening a separate window which doesn't run in Jupyter. You will need to press `q` on that window to close it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# press q to close\n",
    "display_movie = True  # set to False if you want to skip \n",
    "if display_movie:\n",
    "    movie_orig = cm.load(movie_path)  # add subindices here if you want to load partial movie for viewing\n",
    "    downsampling_ratio = 0.2\n",
    "    movie_orig.resize(fz=downsampling_ratio).play(gain=1.3,\n",
    "                                                  q_max=99.5, \n",
    "                                                  fr=30,\n",
    "                                                  plot_text=True,\n",
    "                                                  magnification=4,\n",
    "                                                  backend='opencv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" markdown=\"1\">\n",
    "    <h2 style=\"margin-top: 0;\">Displaying large files</h2>\n",
    "    Loading a movie with <em>cm.load()</em> pulls all of the data into memory, which is not always feasible. When working with your own data, you might need to adapt the above code when working with extremely large files. Caiman provides many tools to handle this use case, which are discussed in <a href=\"#display_large\">Appendix 3: Displaying large files</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also create a couple of summary images of the movie, including a *maximum projection* (the maximum value of each pixel) and a *correlation image* (how correlated each pixel is with its neighbors). If a pixel comes from a neural element (soma, axon, dendrite) it will tend to be highly correlated with its neighbors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_projection_orig = np.max(movie_orig, axis=0)\n",
    "correlation_image_orig = cm.local_correlations(movie_orig, swap_dim=False)\n",
    "correlation_image_orig[np.isnan(correlation_image_orig)] = 0 # get rid of NaNs, if they exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax_max, ax_corr) = plt.subplots(1,2,figsize=(8,4))\n",
    "ax_max.imshow(max_projection_orig, cmap='gray', vmin=0, vmax=700);\n",
    "ax_max.set_title(\"Max Projection Orig\", fontsize=12);\n",
    "\n",
    "ax_corr.imshow(correlation_image_orig, cmap='gray', vmin=0.05, vmax=0.45)\n",
    "ax_corr.set_title('Correlation Image Orig', fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These images will not be particularly sharp yet, as there is still a good deal of motion in the movie. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up parameters\n",
    "In general in Caiman, estimators are first initialized with a set of parameters, and then they are fit against actual data in a separate step. In this section, we'll define a `parameters` object that will subsequently be used to initialize our different estimators. \n",
    "\n",
    "The parameters are divided into different categories. We will not discuss them in detail in this section, but will go over them when needed (and note that in this notebook we will mostly focus on CNMF):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general dataset-dependent parameters\n",
    "fr = 30                     # imaging rate in frames per second\n",
    "decay_time = 0.4            # length of a typical transient in seconds\n",
    "dxy = (2., 2.)              # spatial resolution in x and y in (um per pixel)\n",
    "\n",
    "# motion correction parameters\n",
    "strides = (48, 48)          # start a new patch for pw-rigid motion correction every x pixels\n",
    "overlaps = (24, 24)         # overlap between patches (width of patch = strides+overlaps)\n",
    "max_shifts = (6,6)          # maximum allowed rigid shifts (in pixels)\n",
    "max_deviation_rigid = 3     # maximum shifts deviation allowed for patch with respect to rigid shifts\n",
    "pw_rigid = True             # flag for performing non-rigid motion correction\n",
    "\n",
    "# CNMF parameters for source extraction and deconvolution\n",
    "p = 1                       # order of the autoregressive system (set p=2 if there is visible rise time in data)\n",
    "gnb = 2                     # number of global background components (set to 1 or 2)\n",
    "merge_thr = 0.85            # merging threshold, max correlation allowed\n",
    "bas_nonneg = False          # enforce nonnegativity constraint on calcium traces (technically on baseline)\n",
    "rf = 15                     # half-size of the patches in pixels (patch width is rf*2 + 1)\n",
    "stride_cnmf = 6             # amount of overlap between the patches in pixels (overlap is stride_cnmf+1) \n",
    "K = 4                       # number of components per patch\n",
    "gSig = np.array([4, 4])     # expected half-width of neurons in pixels \n",
    "gSiz = 2*gSig + 1           # half-width of bounding box created around neurons during initialization\n",
    "method_init = 'greedy_roi'  # initialization method (if analyzing dendritic data see demo_dendritic.ipynb)\n",
    "ssub = 1                    # spatial subsampling during initialization \n",
    "tsub = 1                    # temporal subsampling during intialization\n",
    "\n",
    "# parameters for component evaluation\n",
    "min_SNR = 2.0               # signal to noise ratio for accepting a component\n",
    "rval_thr = 0.85             # space correlation threshold for accepting a component\n",
    "cnn_thr = 0.99              # threshold for CNN based classifier\n",
    "cnn_lowest = 0.1            # neurons with cnn probability lower than this value are rejected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We place the above parameter values in a dictionary that is passed to the `CNMFParams` class (those parameters *not* explicitly defined will assume default values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_dict = {'fnames': movie_path,\n",
    "                  'fr': fr,\n",
    "                  'dxy': dxy,\n",
    "                  'decay_time': decay_time,\n",
    "                  'strides': strides,\n",
    "                  'overlaps': overlaps,\n",
    "                  'max_shifts': max_shifts,\n",
    "                  'max_deviation_rigid': max_deviation_rigid,\n",
    "                  'pw_rigid': pw_rigid,\n",
    "                  'p': p,\n",
    "                  'nb': gnb,\n",
    "                  'rf': rf,\n",
    "                  'K': K, \n",
    "                  'gSig': gSig,\n",
    "                  'gSiz': gSiz,\n",
    "                  'stride': stride_cnmf,\n",
    "                  'method_init': method_init,\n",
    "                  'rolling_sum': True,\n",
    "                  'only_init': True,\n",
    "                  'ssub': ssub,\n",
    "                  'tsub': tsub,\n",
    "                  'merge_thr': merge_thr, \n",
    "                  'bas_nonneg': bas_nonneg,\n",
    "                  'min_SNR': min_SNR,\n",
    "                  'rval_thr': rval_thr,\n",
    "                  'use_cnn': True,\n",
    "                  'min_cnn_thr': cnn_thr,\n",
    "                  'cnn_lowest': cnn_lowest}\n",
    "\n",
    "parameters = params.CNMFParams(params_dict=parameter_dict) # CNMFParams is the parameters class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This parameters object (`parameters`) is basically a collection of dictionaries that each contains different categories of parameters. These different dictionaries can be accessed using dot notation.  Some are related to the dataset in general (`parameters.data`), while most are related to specific aspects of the workflow such as motion correction (`parameters.motion`), quality evaluation (`parameters.quality`), and others.\n",
    " \n",
    "For instance, if you want to inspact the dataset-dependent parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access a particular parameter in this parameter field is just to access a value of a dictionary using the appropriate key. So to get the frame rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters.data['fr'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" markdown=\"1\">\n",
    "    <h2 style=\"margin-top: 0;\">To dig deeper into this design</h2>  \n",
    "    To see more about the design of Caiman estimators and parameters, and their decoupling, see <a href=\"#caiman_estimators\">Appendix 4: Estimators and parameters</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a cluster\n",
    "Caiman is optimized for parallel computing, and distributes computations to multiple CPU cores for motion correction and CNMF (there is also an option to run motion correction on the GPU, but we will not focus on that here). Setting up the multicore processing is done with the `setup_cluster()` function below. \n",
    "\n",
    "First, let's see how many CPUs we have available, and set the number of processors we want to use. If you set `num_processors_to_use` to `None`, then `setup_cluster()` will set it one less than the total number available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"You have {psutil.cpu_count()} CPUs available in your current environment\")\n",
    "num_processors_to_use = 8  # None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a cluster of processors. If one has already been set up (the `multiprocessing_pool` variable is already in your namespace), then that cluster will be closed and a new one created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'multiprocessing_pool' in locals():  # 'locals' contains list of current local variables\n",
    "    print('Closing previous cluster')\n",
    "    cm.stop_server(dview=multiprocessing_pool)\n",
    "print(\"Setting up new cluster\")\n",
    "_, multiprocessing_pool, n_processes = cm.cluster.setup_cluster(backend='multiprocessing', \n",
    "                                                 n_processes=num_processors_to_use, \n",
    "                                                 ignore_preexisting=False)\n",
    "print(f\"Successfully set up cluster with {n_processes} processes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't focus much on the outputs, except for `multiprocessing_pool`. This is the multiprocessing pool that will be fed into Caiman's subsequent processing steps. In these later steps, if you set the parameter `dview` to `multiprocessing_pool`, then parallel processing will be used. If instead you set `dview` to `None` then no parallel processing will be used. This latter option can be important when debugging, as the logger doesn't typically work for multithreaded operations.\n",
    "\n",
    "For more details, please see [Appendix 5: Cluster Setup](#the_cluster). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" markdown=\"1\">\n",
    "    <h2 style=\"margin-top: 0;\">Optimizing performance</h2>  \n",
    "If you hit memory issues later, there are a few things you can do. First, you may want to lower the number of processors you are using. Each processor uses more RAM, and on a workstation with many processors, you can sometimes get better performance by reducing <em>num_processors_to_use</em>. Unfortunately, this is a bit of an art form, so the best way to determine the optimal number is by trial and error. When you set <em>num_processors_to_use</em> variable to <em>None</em>, it defaults to <i>one</i> less than the total number of CPU cores available (the reason we don't automatically set it to the total number of cores is because in practice this almost universally leads to worse performance). Also, while you can set <em>num_processors_to_use</em> to a number <i>greater</i> than the available number of CPU cores, we <b>do not</b> recommend it for performance reasons.\n",
    "\n",
    "Second, if your system has less than 32GB of RAM, and things are running slowly or you are running out of memory, then get more RAM. While you can sometimes get away with less, we recommend a *bare minimum* level of 16GB of RAM, but more is better. 32GB RAM is acceptable, 64GB is good, 128GB is great. Obviously, this will depend on the size of your data sets. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motion Correction\n",
    "It is important to get rid of motion artifacts, because the subsequent signal extraction algorithm assumes that each pixel represents the same region of the brain. We initialize the motion correction estimator using the parameters set above:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mot_correct = MotionCorrect(movie_path, dview=multiprocessing_pool, **parameters.motion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook focuses on CNMF, not motion correction, but let's consider a couple of the motion correction parameters we entered above:\n",
    "\n",
    "- `pw_rigid=True` tells us that we are going to perform piecewise rigid motion correction using the nonrigid motion correction (NoRMCorre) algorithm (this is because the data seems to exhibit some non-uniform motion). \n",
    "- NoRMCorre will split the movie into patches that repeat every 48 pixels (`strides`), and have 24 pixels of overlap (`overlaps`): the total patch width is 72 (the sum of stride and overlap)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" markdown=\"1\">\n",
    "    <h2 style=\"margin-top: 0;\">For more on motion correction</h2>  \n",
    "For a detailed analysis of Caiman's motion correction pipeline, see the following demo: <a href=\"./demo_motion_correction.ipynb\">demo_motion_correction.ipynb</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to actually perform motion correction using the `motion_correct()` method on the estimator we built. You may see some warnings about negative movie averages: you can ignore them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#%% Run piecewise-rigid motion correction using NoRMCorre\n",
    "mot_correct.motion_correct(save_movie=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally inspect the results by comparing the original movie: note we are turning the gain up here to highlight motion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compare with original movie  : press q to quit\n",
    "display_movie = True\n",
    "if display_movie:\n",
    "    movie_orig = cm.load(movie_path) # in case it was not loaded earlier\n",
    "    movie_corrected = cm.load(mot_correct.mmap_file) # load motion corrected movie\n",
    "    ds_ratio = 0.2\n",
    "    cm.concatenate([movie_orig.resize(1, 1, ds_ratio) - mot_correct.min_mov*mot_correct.nonneg_movie,\n",
    "                    movie_corrected.resize(1, 1, ds_ratio)], \n",
    "                    axis=2).play(fr=20, \n",
    "                                 gain=2, \n",
    "                                 magnification=2)  # press q to exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the max projection and correlation image of the motion corrected movies. They look quite a bit different than the original movie, more \"crisp\" because they are no longer blurred by movement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_projection = np.max(movie_corrected, axis=0)\n",
    "correlation_image = cm.local_correlations(movie_corrected, swap_dim=False)\n",
    "correlation_image[np.isnan(correlation_image)] = 0 # get rid of NaNs, if they exist\n",
    "\n",
    "f, (ax_max, ax_corr) = plt.subplots(1,2,figsize=(8,4))\n",
    "ax_max.imshow(max_projection, cmap='gray', vmin=300, vmax=1000);\n",
    "ax_max.set_title(\"Max Motion Corrected\", fontsize=12);\n",
    "\n",
    "ax_corr.imshow(correlation_image, cmap='gray', vmin=0.1, vmax=0.4)\n",
    "ax_corr.set_title('Correlation Motion Corrected', fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" markdown=\"1\">\n",
    "    <h2 style=\"margin-top: 0;\">If you don't need motion correction</h2>  \n",
    "If you don't need to run motion correction, for instance if your movie has no movement (which is often the case in slice praparations), you can directly memory map your file to prepare for subsequent  processing:\n",
    "\n",
    "    mc_memmapped_fname = cm.save_memmap(movie_path, base_name='memmap_',\n",
    "                                         order='C', border_to_0=0, dview=multiprocessing_pool)\n",
    "\n",
    "You would obviously need to modify the rest of your code accordingly (e.g., don't run motion correction, don't run the following code cell that creates a memmapped file from motion correction). \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory mapping \n",
    "When you ran motion correction, it saved the motion corrected data as a memory mapped file in `mot_correct.mmap_file`. The cell below saves the same motion corrected data in a second memory mapped file (name stored in `mc_memmapped_fname`) that is optimized for CNMF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "border_to_0 = 0 if mot_correct.border_nan == 'copy' else mot_correct.border_to_0 # trim border against NaNs\n",
    "mc_memmapped_fname = cm.save_memmap(mot_correct.mmap_file, \n",
    "                                        base_name='memmap_', \n",
    "                                        order='C',\n",
    "                                        border_to_0=border_to_0,  # exclude borders, if that was done\n",
    "                                        dview=multiprocessing_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create usable arrays from the memory mapped file we just created. This lets us treat the data as if it were in memory, but leaves it on disk and allows Caiman to perform out-of-core computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yr, dims, num_frames = cm.load_memmap(mc_memmapped_fname)\n",
    "images = np.reshape(Yr.T, [num_frames] + list(dims), order='F') #reshape frames in standard 3d format (T x X x Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have used a caiman built-in function for loading memory mapped file to load `Yr`, which is is the motion corrected movie reshaped so that each frame is in a single column. We have also created a version `images` in more standard movie format (`num_frames x cols x rows`). All of these arrays are memory mapped, so are not loaded into RAM, but reference data on disk and are loaded as needed. \n",
    "\n",
    "Restart the cluster to clean up memory in prep for CNMF run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.stop_server(dview=multiprocessing_pool)\n",
    "_, multiprocessing_pool, n_processes = cm.cluster.setup_cluster(backend='multiprocessing', \n",
    "                                                                n_processes=num_processors_to_use, \n",
    "                                                                single_thread=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_params(cnmf_model):\n",
    "    \"\"\"\n",
    "    Convenience function to return critical parameters given CNMF estimator object.\n",
    "    Returns dictionary with values of rf, stride, gSig, gSiz, K, merge_threshold\n",
    "    \n",
    "    Note: \n",
    "    gSiz is included because it depends on gSig and you want to make sure to change it when you change gSig.\n",
    "    These are not set in stone: tweak for your own needs!\n",
    "    \"\"\"\n",
    "    rf = cnmf_model.params.patch['rf']\n",
    "    stride = cnmf_model.params.patch['stride']\n",
    "    gSig = cnmf_model.params.init['gSig']\n",
    "    gSiz = cnmf_model.params.init['gSiz']\n",
    "    K = cnmf_model.params.init['K']\n",
    "    merge_thr = cnmf_model.params.merging['merge_thr']\n",
    "    \n",
    "    key_params = {'rf': rf, \n",
    "                  'stride': stride,\n",
    "                  'gSig': gSig,\n",
    "                  'gSiz': gSiz,\n",
    "                  'K': K,\n",
    "                  'merge_thr': merge_thr}\n",
    "    \n",
    "    return key_params\n",
    "\n",
    "print('loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.typing import ArrayLike\n",
    "from typing import Any, Optional, Tuple\n",
    "\n",
    "def get_rectangle_coords(im_dims: ArrayLike, \n",
    "                         stride: int, \n",
    "                         overlap: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Extract rectangle (patch) coordinates: a helper function used by view_quilt().\n",
    "    \n",
    "    Given dimensions of summary image (rows x colums), stride between patches, and overlap\n",
    "    between patches, returns row coordinates of the patches in patch_rows, and column \n",
    "    coordinates patches in patch_cols. This is meant to be used by plot_patches().\n",
    "       \n",
    "    Args:\n",
    "        im_dims: array-like\n",
    "            dimension of image (num_rows, num_cols)\n",
    "        stride: int\n",
    "            stride between patches in pixels\n",
    "        overlap: int\n",
    "            overlap between patches in pixels\n",
    "    \n",
    "    Returns:\n",
    "        patch_rows: ndarray\n",
    "            num_patch_rows x 2 array, where row i contains onset and offset row pixels for patch row i\n",
    "        patch_cols: ndarray\n",
    "            num_patch_cols x 2 array, where row j contains onset and offset column pixels for patch column j\n",
    "            \n",
    "    Note: \n",
    "        Currently assumes square patches so takes in a single number for stride/overlap. \n",
    "    \"\"\"\n",
    "    patch_width = overlap + stride\n",
    "    \n",
    "    patch_onset_rows = np.array(list(range(0, im_dims[0] - patch_width, stride)) + [im_dims[0] - patch_width])\n",
    "    patch_offset_rows = patch_onset_rows + patch_width\n",
    "    patch_offset_rows[patch_offset_rows > im_dims[0]-1] = im_dims[0]-1\n",
    "    patch_rows = np.column_stack((patch_onset_rows, patch_offset_rows))\n",
    "    \n",
    "    patch_onset_cols = np.array(list(range(0, im_dims[1] - patch_width, stride)) + [im_dims[1] - patch_width])\n",
    "    patch_offset_cols = patch_onset_cols + patch_width \n",
    "    patch_offset_cols[patch_offset_cols > im_dims[1]-1] = im_dims[1]-1\n",
    "    patch_cols = np.column_stack((patch_onset_cols, patch_offset_cols))\n",
    "    \n",
    "    return patch_rows, patch_cols\n",
    "\n",
    "\n",
    "def rect_draw(row_minmax: ArrayLike, \n",
    "              col_minmax: ArrayLike, \n",
    "              color: Optional[str]='white', \n",
    "              alpha: Optional[float]=0.3, \n",
    "              ax: Optional[Any]=None) -> Tuple[Any, Any]:\n",
    "    \"\"\"\n",
    "    Draw a single transluscent rectangle on given axes object.\n",
    "    \n",
    "    Args:\n",
    "        row_minmax: array-like\n",
    "            [row_min, row_max] -- 2-elt int bounds for rect rows\n",
    "        col_minmax: array-like \n",
    "            [col_min, col_max] -- int bounds for rect cols\n",
    "        color : string\n",
    "            rectangle color, default 'yellow'\n",
    "        alpha : float\n",
    "            rectangle alpha (0. to 1., where 1 is opaque), default 0.3\n",
    "        ax : pyplot.Axes object\n",
    "            axes object upon which rectangle will be drawn, default None\n",
    "    \n",
    "    Returns:\n",
    "        ax (Axes object)\n",
    "        rect (Rectangle object)\n",
    "    \"\"\"\n",
    "    from matplotlib.patches import Rectangle\n",
    "    \n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "        \n",
    "    box_origin = (col_minmax[0], row_minmax[0])\n",
    "    box_height = row_minmax[1] - row_minmax[0] \n",
    "    box_width = col_minmax[1] - col_minmax[0]\n",
    "\n",
    "    rect = Rectangle(box_origin, \n",
    "                     width=box_width, \n",
    "                     height=box_height,\n",
    "                     color=color, \n",
    "                     alpha=alpha)\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "    return ax, rect\n",
    "\n",
    "\n",
    "def view_quilt(template_image: np.ndarray, \n",
    "               stride: int, \n",
    "               overlap: int, \n",
    "               color: Optional[str]='white', \n",
    "               alpha: Optional[float]=0.3, \n",
    "               vmin: Optional[float]=0.0, \n",
    "               vmax: Optional[float]=0.6, \n",
    "               figsize: Optional[Tuple]=(6,6)) -> Any:\n",
    "    \"\"\"\n",
    "    Plot patches on template image given stride and overlap parameters. \n",
    "    \n",
    "    Given stride and overlap, plot overlapping patch blocks on template image.\n",
    "    This can be useful for checking motion correction and cnmf spatial parameters. \n",
    "    It ends up looking like a quilt pattern.\n",
    "    \n",
    "    Args:\n",
    "        template_image: ndarray\n",
    "            row x col summary image upon which to draw patches (e.g., correlation image)\n",
    "        stride (int) stride between patches in pixels\n",
    "        overlap (int) overlap between patches in pixels\n",
    "        color (str): color of patches, default 'white'\n",
    "        alpha (float) : patch transparency (0. to 1.: higher is more opaque), default 0.3\n",
    "        vmin (float) : vmin for plotting underlying template image, default None\n",
    "        vmax (float) : vmax for plotting underlying template image, default None\n",
    "        figsize tuple : fig size in inches (width, height), default (6.,6.)\n",
    "    \n",
    "    Returns:\n",
    "        ax: axes object\n",
    "    \n",
    "    Example:\n",
    "        # Uses cnm object (cnm) and correlation image (corr_image) as template:\n",
    "        patch_width = 2*cnm.params.patch['rf'] + 1\n",
    "        patch_overlap = cnm.params.patch['stride'] + 1\n",
    "        patch_stride = patch_width - patch_overlap\n",
    "        ax = plot_patches(corr_image, patch_stride, patch_overlap, vmin=0.0, vmax=0.6);\n",
    "        \n",
    "    Note: \n",
    "        Currently assumes square patches so takes in a single number for stride/overlap.\n",
    "    \"\"\"\n",
    "    im_dims = template_image.shape\n",
    "    patch_rows, patch_cols = get_rectangle_coords(im_dims, stride, overlap)\n",
    "    \n",
    "    f, ax = plt.subplots(figsize=figsize)\n",
    "    ax.imshow(template_image, cmap='gray', vmin=vmin, vmax=vmax)\n",
    "    for patch_row in patch_rows:\n",
    "        for patch_col in patch_cols:\n",
    "            #print(f\"row: {patch_row}, col {patch_col}\")\n",
    "            ax, _ = rect_draw(patch_row, patch_col, color='white', alpha=0.2, ax=ax)\n",
    "            \n",
    "    return ax\n",
    "\n",
    "print('loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run CNMF on patches in parallel\n",
    "\n",
    "Everything is now set up for running the constrained nonnegative matrix factorization (CNMF) algorithm. As shown in the figure in the introduction, this algorithm simultaneously extracts the *spatial footprint* of each neural component and the corresponding *denoised calcium signal* for each component. It also performs *deconvolution*, providing an estimate of the spike count that generated the calcium signal in the movie. These estimates will be available in an `estimates` class that we will discuss much more below.\n",
    "\n",
    "The algorithm is parallelized as illustrated here:\n",
    "\n",
    "![cnmf patch flow image](../../docs/img/cnmf_patches.jpg)\n",
    "\n",
    "1) The movie field of view is split into overlapping patches.\n",
    "2) These patches are processed in parallel by the CNMF algorithm. The degree of parallelization depends on your available computing power: if you have just one CPU then the chunks will be processed sequentially. \n",
    "3) The results from all the patches are merged, with special focus on components in overlapping regions -- overlapping components are merged if their activity is highly correlated.\n",
    "4) Results are refined with additional iteratoins of CNMF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed above, Caiman's main algorithms are run in two steps: first the estimators are *initialized* with a set of parameters, and then they are *fit* against actual data. Let's create our CNMF estimator object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnmf_model = cnmf.CNMF(n_processes, \n",
    "                       params=parameters, \n",
    "                       dview=multiprocessing_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once initialized, we we could immediately run `cnmf_model.fit(images)` if we knew the parameters were good. The parameters in your CNMF estimator object are accessable in `cnmf_model.params`. However, before running the algorithm, let's discuss the more critical parameters that you will most likely need to change in the future when running CNMF on your own data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key parameters for CNMF\n",
    "\n",
    "`rf (int)`: *patch half-width*\n",
    "\n",
    "> `rf`, which stands for 'receptive field', is the half width of patches in pixels. The full patch width is `2*rf + 1`. `rf` should be *at least* 3-4 times larger than the expected neuron diameter. This is so that the CNMF algorithm will be able to properly estimate local background activity. Also the larger the patch size, the less parallelization will be used by Caiman. If `rf` is set to `None`, then no parallelization will be used and CNMF will be run on the entire field of view. \n",
    "\n",
    "`stride_cnmf (int)`: *patch overlap*\n",
    "\n",
    "> `stride_cnmf` is the overlap between patches in pixels (the actual overlap is `stride_cnmf + 1`). This should be at least the diameter of a neuron. The larger the overlap, the greater the computational load, but the results will be more accurate when stitching together results from different patches. This param should probably be called 'overlap' instead of 'stride'. \n",
    " \n",
    " \n",
    "`gSig (int, int)`: *half-width of neurons*\n",
    "\n",
    "> `gSig` is roughly the half-width of neurons in your movie in pixels (height, width). Used during initialization, technically it is the sigma parameter in a Gaussian filter run on all the images. If the filter is appropriately matched to your data, you will get a much better estimate, so this is one of the most important parameters. It also determines the value of `gSiz`, which is set to `2*gSig + 1` for CNMF. `gSiz` is the width of a bounding box created around each seed pixel during initilialization -- if after running `refit()` below your neurons end up looking square or artificially cut off, you may need to increase `gSiz`.\n",
    "    \n",
    "    \n",
    "`K (int)`: *components per patch*\n",
    "\n",
    "> `K` is the number of components you expect per patch. You should obviously adapt this to the density of components in your data, and the current `rf` parameter. We suggest pick `K` based on the more dense patches in your movie, as it is easy to remove false positives later in the component evaluation stage. However, if you pick `K` much too high, the algorithm will take longer to run, and the false positives will become inconvenient to sift through later.\n",
    "    \n",
    "`merge_thr (float)`: *merge threshold* \n",
    "\n",
    "> `merge_thr` is the merge threshold. If two spatially overlapping components are correlated above this value, they will be merged into one component. The correlation coefficient is calculated using their respective calcium traces.  \n",
    "\n",
    "You typically will set `rf` and `stride` infrequently, so it is mainly `K`, `gSig`, and `merge_thr` that you will tweak when analyzing a given session if things are looking awry. Note these are not the *only* important parameters. They just tend to be the *most* important, while many others tend to depend on your calcium indicator or other factors that don't vary within an experiment, and are listed above in the parameter setting section and [in the docs](https://caiman.readthedocs.io/en/master/Getting_Started.html#parameters).\n",
    "\n",
    "It is useful to know the original values of the key parameters, and we can access them using the helper function `key_params()` (we include `gSiz` in this list just so that if you tweak `gSig`, you will remember to change `gSiz` as well):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Initial key params: {key_params(cnmf_model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to pick parameters\n",
    "How do you determine what values to use for the key parameters with your own data? The previous section provided some guidelines on how to select them via inspection of your data. Basically: look at your movie, or a summary image for your movie, and pick values close to those suggested by the guidelines above. There is typically no *perfect* value, and there will usually be some trial-and-error involved: luckily the models are fairly robust to small changes in their values.\n",
    "\n",
    "It is helpful to use `view_quilt()` function to see if our critical spatial parameters are in the right ballpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate stride and overlap from parameters\n",
    "cnmf_patch_width = cnmf_model.params.patch['rf']*2 + 1\n",
    "cnmf_patch_overlap = cnmf_model.params.patch['stride'] + 1\n",
    "cnmf_patch_stride = cnmf_patch_width - cnmf_patch_overlap\n",
    "print(f'Patch width: {cnmf_patch_width} , Stride: {cnmf_patch_stride}, Overlap: {cnmf_patch_overlap}');\n",
    "\n",
    "# plot the patches\n",
    "patch_ax = view_quilt(correlation_image, \n",
    "                      cnmf_patch_stride, \n",
    "                      cnmf_patch_overlap, \n",
    "                      vmin=0.1, \n",
    "                      vmax=0.5, \n",
    "                      figsize=(6,6));\n",
    "patch_ax.set_title(f'CNMF Patches Width {cnmf_patch_width}, Overlap {cnmf_patch_overlap}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate spatial parameters using the quilt plot\n",
    "- Is the patch width (`2*rf+1`) at least three times the width of a neuron? Yes\n",
    "- Do individual neurons fit in the overlap region (`stride`)? No, current value of 6 is a bit low, so let's bump `stride` to 9 or 10.\n",
    "- When in interactive (`qt` or `notebook`) mode, you can zoom and inspect the average width of each neuron in pixels. Is `gSig` about half that? Yes. Each neuron is about 6-8 pixels wide, and `gSig` is 4.\n",
    "- For `K`: how many neurons are in each patch? Here you want an upper bound, not an average. The current value of 4 seems good.\n",
    "\n",
    "If you can't remember the current parameter values, remember you can always use the `key_params()` convenience function to print them out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_params(cnmf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the only parameter we need to change is `stride`. To *change* parameters, Caiman has a built-in `change_params()` function, and you can iteratively go through the following cell, tweaking params until you are happy with them based on the `rf/stride/K/gSig` combination for your data. \n",
    "\n",
    "Note while you can just change the parameter and keep going, we recommend inspecting the patches that will result using `plot_patches()`, as there can be counterintuitive edge effects with patches that yield a lot of redundancy of computation with certain combinations of `rf` and `stride`, so it is helpful to inspect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_new = rf\n",
    "stride_new = 10\n",
    "gsig_new = gSig # unchanged\n",
    "gsiz_new = gSig*2 + 1\n",
    "k_new = K  # unchanged\n",
    "merge_thr_new = merge_thr  # unchanged\n",
    "\n",
    "print(f\"Before changing: {key_params(cnmf_model)}\")\n",
    "cnmf_new_params = {'rf': rf_new,\n",
    "                   'stride': stride_new,\n",
    "                   'gSig': gsig_new,\n",
    "                   'gSiz': gsiz_new, \n",
    "                   'K': k_new,\n",
    "                   'merge_thr': merge_thr_new}\n",
    "cnmf_model.params.change_params(params_dict=cnmf_new_params)\n",
    "\n",
    "\n",
    "# plot the patches\n",
    "# calculate stride and overlap from parameters\n",
    "cnmf_overlap = cnmf_model.params.patch['stride'] + 1\n",
    "cnmf_patch_width = cnmf_model.params.patch['rf']*2 + 1\n",
    "cnmf_stride = cnmf_patch_width - cnmf_overlap\n",
    "\n",
    "patch_ax = view_quilt(correlation_image, \n",
    "                        cnmf_stride, \n",
    "                        cnmf_overlap, \n",
    "                        vmin=0.1,\n",
    "                        vmax=0.5, \n",
    "                        figsize=(6,6));\n",
    "patch_ax.set_title(f'CNMF Patches Width {cnmf_patch_width}, Overlap {cnmf_overlap}');\n",
    "\n",
    "\n",
    "print(f\"After changing: {key_params(cnmf_model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new parameters look good, but ultimately the only way to really know is to fit the model with the parameters and see how it does: often you have to iteratively search a bit in parameter space. It's time to run CNMF! Note if you are ever unhappy with your parameters you can always run through an iterative process like the one above to improve them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" markdown=\"1\">\n",
    "    <h2 style=\"margin-top: 0;\">Mesmerize your search!</h2>  \n",
    "It is not always efficient to DIY such iterative parameter search, especially if you end up needing to do a grid search in a large parameter space. <a href=https://github.com/nel-lab/mesmerize-core>Mesmerize</a> is a package built for parameter exploration in Caiman (both for motion correction and the CNMF algorithms) that keeps track of all your different results, and includes powerful GPU-based tools for visualization of results in Jupyter notebooks. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run CNMF\n",
    "Now that we are happy with our parameters, let's run the cnmf algorithm using the `fit()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cnmf_fit = cnmf_model.fit(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" markdown=\"1\">\n",
    "    <h2 style=\"margin-top: 0;\">Run all of the above with one command</h2>  \n",
    "It is possible to run the combined steps of motion correction, memory mapping, and cnmf fitting in one step using the <em>fit_file()</em> method. We recommend that you familiriaze yourself with the different steps first. It can be useful when testing code, or if sending jobs to a cluster in those cases when you already know the parameter settings.\n",
    "\n",
    "    cnmf_all = cnmf.CNMF(n_processes, \n",
    "                         params=parameters, \n",
    "                         dview=multiprocessing_pool)\n",
    "    cnmf_all.fit_file(motion_correct=True)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the initial estimates\n",
    "Briefly inspect the results by plotting contours of identified components using `plot_contours_nb()`. \n",
    "\n",
    "You can interactively explore this plot in your notebook with the help of the buttons on the right-hand-side of the plot (it was made using the [Bokeh](https://bokeh.org/) library)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnmf_fit.estimates.plot_contours_nb(img=correlation_image, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note this is just an initial result, which will contain many false positives, which is to be expected. The main concern to watch for here is whether you have lots of false *negatives* (has the algorithm missed neurons?). False negatives are hard to fix later, so if you have an unacceptable number, be sure to go back and re-run CNMF with new parameters.\n",
    "\n",
    "> If you have a large number of neurons -- you may get a data rate error with the previous plotting command. If that happens, you can start your notebook using `jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-run (seeded) CNMF  on the full field of view \n",
    "It can often be helpful to refine the initial estimates by re-running the CNMF algorithm seeded just on the estimates from the previous step using the `refit()` method. This can be especially useful if the initial estimates appear unusual: it is often a sign that the algorithm hasn't converged on a solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cnmf_refit = cnmf_fit.refit(images, dview=multiprocessing_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spatial contours of the new estimates typically look cleaner and more \"neuronal\" after running `refit()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnmf_refit.estimates.plot_contours_nb(img=correlation_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that the estimates look qualitative better, but we still expect false positives. The **component evaluation** stage of the pipeline will evaluate the initial set of estimates, and remove the bad ones. But first, let's disuss the estimates object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The estimates class\n",
    "The main point of the CNMF algorithm was to perform source separation, to extract the spatial footprints and temporal traces of calcium activity from neurons that were hiding in the raw data. This information, and many useful methods for visualization and analysis, are contained in Caiman's `Estimates` class. An `estimates` object was generated as an attribute of your CNMF model after running `fit()` or `refit()` (e.g., you can find it in `cnmf_refit.estimates`). \n",
    "\n",
    "The rest of this notebook, from component evaluation to calculation of DFoF, is effectively an exploration of the properties and methods of the `Estimates` class. Because of its centrality, it is crucial to understand some of the basic attributes of `estimates` when working with Caiman. The most important estimates generated are:\n",
    "\n",
    "    C: denoised calcium traces (num components x num frames) -- the temporal traces\n",
    "    A: spatial components (num pixels x num components) - the spatial footprints\n",
    "    YrA: residual for each calcium trace (num components x num frames)\n",
    "    S: spike count estimate from deconvolution, if used (num components x num frames)\n",
    "    F_dff: deltaF/F -- detrended and normalized raw calcium traces (num components x num frames)\n",
    "\n",
    "To recover raw calcium traces, you can add together the denoised calcium traces and their residuals (`C + YrA`). Note also the `F_dff` calculation is not done automatically, so when you run `fit()/refit()` this field will initially be `None`. We show how to calculate it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see shape of A and C\n",
    "cnmf_refit.estimates.A.shape, cnmf_refit.estimates.C.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" markdown=\"1\">\n",
    "    <h2 style=\"margin-top: 0;\">More on Estimates</h2>  \n",
    "The estimates objects contain a great deal of information. The attributes are discussed in more detail <a href=\"https://github.com/flatironinstitute/CaImAn/wiki/Interpreting-Results\">in the documentation</a>, but you might also find exploring the <a href=\"https://github.com/flatironinstitute/CaImAn/blob/main/caiman/source_extraction/cnmf/estimates.py\">source code</a> rewarding. For instance, while most users initially care about the extracted calcium signals <em>C</em> and spatial footprints <em>A</em>, the <b>background noise</b> is an important part of the model in Caiman, and this is also part of the estimate, in fields <em>b</em> and <em>f</em> (which correspond to the spatial and temporal components of the low-rank background model, respectively). \n",
    "\n",
    "Note we realize these attribute names like <em>A</em> are not very informative or Pythonic. They are rooted in mathematical conventions from the original papers, but we will likely change them to be more useful to users and readers of code. However, this will be a fairly major breaking change to Caiman, so we will give a great deal of advance warning before Caiman 2.0 releases with such changes. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Component Evaluation\n",
    "As already mentioned, the initial estimates produced by CNMF creates many spurious components. Our next step is to do some quality control, evaluating each component with the `evaluate_components()` method. The criteria Caiman uses to evaluate components are:  \n",
    "\n",
    "- **Signal to noise ratio (SNR)**: a baseline noise estimate is extraced for each raw calcium trace, and SNR during calcium transients is calculated relative to this baseline. It is stored in `estimates.SNR_comp`. Those components with high SNR are higher quality, and less likely to be false positives.\n",
    "- **Spatial correlation**: the extracted spatial footprints in `estimates.A` should be highly correlated with the fluctuations of activity in the raw movie, at least on those frames when that component is active. The correlation coefficient between an extracted spatial component and calcium fluctuations in a region of the raw data file are stored in `estimates.r_values`. \n",
    "- **CNN confidence**: Each spatial component in `estimates.A` is passed through a CNN-based classifier that produces a confidence value between 0 and 1 that the shape is a real neuron. These are stored in `estimates.cnn_preds`.\n",
    "\n",
    "The first two criteria are illustrated schematically here (see also Figure 2 of <a href=\"https://elifesciences.org/articles/38173\">the Caiman paper</a>):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![component evaluation image](../../docs/img/component_evaluation.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caiman's compoonent evaluation step is run using the `evaluate_components()` method, which uses the above criteria to sort components into accepted and rejected components. For each criterion, there is a threshold value pulled from the `quality` field of the parameters object in `min_SNR`, `rval_thr`, and `min_cnn_thr`, respectively. If a unit is below *all* of those threshold values, it will be rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Thresholds to be used for evaluate_components()\")\n",
    "print(f\"min_SNR = {cnmf_refit.params.quality['min_SNR']}\")\n",
    "print(f\"rval_thr = {cnmf_refit.params.quality['rval_thr']}\")\n",
    "print(f\"min_cnn_thr = {cnmf_refit.params.quality['min_cnn_thr']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `estimates.evaluate_components()` to sort components into accepted/rejected (this can take some time if you have a great deal of components):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnmf_refit.estimates.evaluate_components(images, cnmf_refit.params, dview=multiprocessing_pool);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method filled in two arrays in the `estimates` class: `idx_components` (accepted) and `idx_components_bad` (rejected). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Num accepted/rejected: {len(cnmf_refit.estimates.idx_components)}, {len(cnmf_refit.estimates.idx_components_bad)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" markdown=\"1\">\n",
    "    <h2 style=\"margin-top: 0;\">More on component evaluation</h2>  \n",
    "In practice, SNR is the most important evaluation factor. The spatial correlation factors are less important. In particular, the CNN for spatial evaluation may be inaccurate if your neural components are not \"canonically\" shaped somata. \n",
    "\n",
    "\n",
    "You may have noticed from the description above, that when running <em>evaluate_components()</em> the three evaluation thresholds are appied <em>disjunctively</em>: if a component is above <em>any</em> of the thresholds, it will pass muster. This was found in practice to be reasonable (e.g., a low SNR component that is very strongly neuronally shaped tends to not be an accident: it is just a very low SNR neuron). However, there is a second set of <b>absolute</b> threshold parameters you can set for each criterion. If a component is <em>below</em> this absolute threshold for any of the evaluation parameters, it will simply be discarded: these are the <em>SNR_lowest</em>, <em>rval_lowest</em>, and <em>cnn_lowest</em> thresholds, respectively. You can think of these as short-circuits that throw components into the incinerator. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing results\n",
    "We've sorted our components into accepted/rejected, so it's time to start looking at the results!\n",
    "\n",
    "There are many built-in `estimates` methods for visualizaing results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All contours\n",
    "We have already used `plot_contours_nb()`, but if we provide it the `idx` keyword it will split the view into accepted and rejected components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnmf_refit.estimates.plot_contours_nb(img=correlation_image, idx=cnmf_refit.estimates.idx_components);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View individual components\n",
    "Using `nb_view_components()`, you can view the spatial and temporal components (A/C). This tool also conveniently displays  the three evaluation criteria values for each component, so you can get a better sense for how the quality maps onto each component. This can be useful if you feel you need to change your evaluation criteria and re-run `evaluate_components()`. Perhaps you have too many false negatives and want to lower your SNR threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accepted components\n",
    "cnmf_refit.estimates.nb_view_components(img=correlation_image, \n",
    "                                        idx=cnmf_refit.estimates.idx_components,\n",
    "                                        cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the above plots the raw traces (`C+YrA`) by default, but you can superimpose the denoised traces from `C` if you add a color to the `denoised_color` parameter. As always in Jupyter, if you are unsure how a method works, you can enter `cnmf_refit.estimates.nb_view_components?` in a new cell to get the documentation for the method. \n",
    "\n",
    "We can also view the rejected compoonents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rejected components\n",
    "if len(cnmf_refit.estimates.idx_components_bad) > 0:\n",
    "    cnmf_refit.estimates.nb_view_components(img=correlation_image, \n",
    "                                            idx=cnmf_refit.estimates.idx_components_bad, \n",
    "                                            cmap='gray',\n",
    "                                            denoised_color='red')\n",
    "else:\n",
    "    print(\"No components were rejected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note one legacy from Matlab with these plotters is that they use one-based indexing when showing `Neuron number`. This can make it confusing when comparing to your own plotting results which will use zero-based indexing. We will either change this, or make it a parameter you can tweak in the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building your own visualizations\n",
    "> This is a slightly more advanced section that you can safely skip your first time through. \n",
    "\n",
    "The main goal with Caiman is to process your raw data and extract a good set of Calcium traces, so you can then go do your analysis, write your paper etc. However, there are many custom visualizations you can build yourself based on the estimates generated from Caiman. \n",
    "\n",
    "For instance, say you want to plot the spatial footprint of a neuron, with the contour superimposed.  The contours of the spatial footprints are in `estimates.coordinates`, which is a list, each containing a dictionary corresponding to a component. One key of the dict includes `coordinates`, which contains the x,y coordinates of the contour that you can plot superimposed on the corresponding column of `A`, the spatial footprint. \n",
    "\n",
    "The following extracts all of the contours of the accepted components into a list from the estimates (note it puts them in a list because they are not all the same size):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_accepted = cnmf_refit.estimates.idx_components\n",
    "all_contour_coords = [cnmf_refit.estimates.coordinates[idx]['coordinates'] for idx in idx_accepted]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, to plot the contour with an individual footprint (which is stored as a linearized compressed sparse column array), you can convert the footprint back to a dense array (with `toarray()`), reshape it to a 2d array, and then plot the footprint with superimpose its contour:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_plot = 30\n",
    "component_contour = all_contour_coords[idx_to_plot]\n",
    "component_footprint = np.reshape(cnmf_refit.estimates.A[:, idx_accepted[idx_to_plot]].toarray(), dims, order='F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(); \n",
    "plt.imshow(component_footprint, cmap='gray');\n",
    "plt.plot(component_contour[:, 0], \n",
    "         component_contour[:, 1], \n",
    "         color='pink', \n",
    "         linewidth=2)\n",
    "plt.title('Footprint/Contour');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fastplotlib side blurb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## How to save and load results (optional)\n",
    "There is a built-in `save()` method for the `cnmf` object, which will save an hdf5 file with all of its properties. This will let you reload the object later so you don't have to go through all the above steps again. Here we save it to the current working directory, but you can give `save()` a full absolute path to control where it saves in your application. \n",
    "\n",
    "> Note: when you save, you are only saving what is contained in the `cnmf` object. If you want to save other things in your workspace, one option os to save things on your own using the `joblib` or `pickle` packages. Alternatively, you can attach things like the correlation image to the `estimates` object, so that it will be loaded with the `cnmf` object. We'll show how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results = True\n",
    "if save_results:\n",
    "    save_path =  r'demo_pipeline_results.hdf5'  # or add full/path/to/file.hdf5\n",
    "    cnmf_refit.estimates.Cn = correlation_image # squirrel away correlation image with cnmf object\n",
    "    cnmf_refit.save(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading results once they have been saved\n",
    "Once you have saved the results, how do you save them? There is a `load_CNMF()` you can use to load results that were stored using `CNMF.save()`. This can be crucial for setting restore points so you don't have to run your analysis again. If your data was saved to `save_path`, you can resuscitate the `cnmf_refit` object with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_results = False\n",
    "if load_results:\n",
    "    cnmf_refit = cnmf.load_CNMF(save_path, \n",
    "                                n_processes=num_processors_to_use, \n",
    "                                dview=multiprocessing_pool)\n",
    "    correlation_image = cnmf_refit.estimates.Cn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A few final things\n",
    "We have extracted the calcium traces and spatial footprints, but there are a few loose ends remaining. One, many people want to calculate dfof, or the calcium fluorescence relative to some baseline value. This is its own fairly complex topic ()\n",
    "\n",
    "### Extract $\\Delta F/F$ values\n",
    "As discussed above, $\\Delta F/F$, the detrended and normalized calcium traces, are not extracted during the CNMF source separation step. The current traces in `C` are in arbitrary units. To convert to normalized (unitless) dfof measure, which is fairly common for publication (it roughly measures deviation from baseline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cnmf_refit.estimates.F_dff is None:\n",
    "    cnmf_refit.estimates.detrend_df_f(quantileMin=8, \n",
    "                                       frames_window=250);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select only high quality components (optional)\n",
    "If you wish to discard the rejected components from the `estimates` field, and keep only the accepted components, you can run the following. This can be useful if you are sure you only want to focus on the accepted components. If you select the \n",
    "\n",
    "use-objects explain\n",
    "explain keep-discarded etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnmf_refit.estimates.select_components?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnmf_refit.estimates.select_components(use_object=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display final results\n",
    "\n",
    "Explain this!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnmf_refit.estimates.nb_view_components(img=correlation_image, denoised_color='red')\n",
    "print('you may need to change the data rate to generate this one: use jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10 before opening jupyter notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing, saving, and creating denoised version\n",
    "\n",
    "Todo: what does that even mean?    \n",
    "Todo: how to open? Show the save/open way sooner.   \n",
    "\n",
    "### You can save an hdf5 file with all the fields of the cnmf object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results = False\n",
    "if save_results:\n",
    "    cnmf_refit.save('analysis_results.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop cluster and clean up log files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% STOP CLUSTER and clean up log files\n",
    "cm.stop_server(dview=multiprocessing_pool)\n",
    "log_files = glob.glob('*_LOG_*')\n",
    "for log_file in log_files:\n",
    "    os.remove(log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View movie with the results\n",
    "This is super important explain it!\n",
    "\n",
    "We can inspect the denoised results by reconstructing the movie and playing alongside the original data and the resulting (amplified) residual movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnmf_refit.estimates.play_movie(images, q_max=99.9, gain_res=2,\n",
    "                                  magnification=2,\n",
    "                                  bpx=border_to_0,\n",
    "                                  include_bck=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The denoised movie can also be explicitly constructed using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% reconstruct denoised movie\n",
    "denoised = cm.movie(cnmf_refit.estimates.A.dot(cnmf_refit.estimates.C) + \\\n",
    "                    cnmf_refit.estimates.b.dot(cnmf_refit.estimates.f)).reshape(dims + (-1,), order='F').transpose([2, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=logging_explained></a>\n",
    "## Appendix 1: Logging\n",
    "Python has a powerful built-in [logging module](https://docs.python.org/3/library/logging.html) for generating log messages while a program is running. It lets you generate custom log messages, and set a threshold to determine which logs you will see. You will only receive messages above the severity threshold you set: you can choose from: `logging.DEBUG`, `logging.INFO`, `logging.WARNING`, `logging.ERROR`, or `logging.CRITICAL`. For instance, setting the threshold to `logging.DEBUG` will print out every logging statement, while setting it to `logging.ERROR` will print out only errors and critical messages. This system gives much more flexibility and control than interspersing `print()` statements throughought your code when debugging. \n",
    "\n",
    "Our custom formatted log string is defined in the `log_format` parameter below, which draws from a predefined [set of attributes](https://docs.python.org/3/library/logging.html#logrecord-attributes) provided by the logging module. We have set each log to display the time, severity level, filename/function name/line number of the file creating the log, process ID, and the actual log message. \n",
    "\n",
    "While logging is especially helpful when running code on a server, it can also be helpful to get feedback in real time on your personal machine, either to audit progress or diagnose problems when debugging. If you set this feature up by running the following cell, the logs will by default go to console. If you want to direct your log to file (which you can indicate with `use_logfile = True`), then it will automatically be directed to your `caiman_data/temp` directory as defined in the `caiman.paths` module. You can set another path with the `filename` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_logfile = False # If set to True, will log to file\n",
    "if use_logfile:\n",
    "    log_file = Path(cm.paths.get_tempdir()) / 'cnmf_demo.log' # \n",
    "    print(f\"Will save logging data to {tmp_file}\")\n",
    "else:\n",
    "    log_file = None\n",
    "log_format = \"{asctime} - {levelname} - [{filename} {funcName}() {lineno}] - pid {process} - {message}\"\n",
    "logging.basicConfig(format=log_format,\n",
    "                    filename=log_file, \n",
    "                    level=logging.WARNING, style=\"{\") #DEBUG, INFO, WARNING, ERROR, CRITICAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "   \n",
    "Once you have set up your logging configuration, you can change the level (say, from `WARNING` to `DEBUG`) using the following: `logging.getLogger().setLevel(logging.DEBUG)`. \n",
    "    \n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=multiple_files></a>\n",
    "## Appendix 2: Working with multiple files\n",
    "Many acquisition systems break up data from a single continuous session across multiple files. It is relatively easy to adapt the current demo to work with multiple files so you can see what the workflow would be like. There are a couple of changes you would need to make. First, instead of just downloading a single demo file, you would want to create a *list* of files that would later be treated as a single continuous data stream. While in the main demo, we have a `movie_path` variable that contains a single path, in this case we want to create a *list* of such paths in a `movie_paths` variable. We have split the main demo movie into two `Sue_split1.tif` and `Sue_split2.tif` to show how this can be done using similar mechanisms to the main one-file pipeline:\n",
    "\n",
    "    movie_path1 = download_demo('Sue_Split1.tif')\n",
    "    movie_path2 = download_demo('Sue_Split2.tif')\n",
    "    movie_paths = [movie_path1, movie_path2]\n",
    "\n",
    "Then, when creating the movie object instead of using `cm.load()` you would use `cm.load_movie_chain()` which takes in a list as an argument:\n",
    "\n",
    "    movie_orig = cm.load_movie_chain(movie_paths)\n",
    "\n",
    "If your data is too large to fit in RAM and you only want to load a subset, please see Appendix 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "   \n",
    "If you have <b>noncontiguous</b> recording sessions, for instance data in files from sessions separated by many days or weeks, and you need to register/match the neurons from these sessions, this is a different use case. We do have a demo for that: see [demo_multisession_registration.ipynb](./demo_multisession_registration.ipynb).\n",
    "    \n",
    "</div> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=display_large></a>\n",
    "## Appendix 3: Displaying large files\n",
    "\n",
    "[Add play_movie! and cv_inline or whatever, also, show how to save movies]\n",
    "\n",
    "Loading movie objects requires loading all of the data you want to view into memory, and this will not be possible with extremely large data sets. But even with very large data sets, you typically want to visualize what is going on, make sure things seem reasonable, etc. Caiman has built-in tools to just load some of a movie into a movie object using the `subindices` argument to the `load()` function. For example, if you just want to load the first 500 frames of a movie, you can send it `subindices=np.arange(0,500)`. \n",
    "\n",
    "Saving movies:\n",
    "\n",
    "    movie_name = r'C:/Users/Eric/Videos/Captures/saved_corr.mp4'\n",
    "    save_codec = 'mp4v'\n",
    "    save_movie = True\n",
    "    movie_orig = cm.load(movie_path) # in case it was not loaded earlier\n",
    "    movie_corrected = cm.load(mot_correct.mmap_file) # load motion corrected movie\n",
    "    print('done')\n",
    "\n",
    "\n",
    "    #%% compare with original movie\n",
    "    display_movie = True\n",
    "    if display_movie:\n",
    "        ds_ratio = 0.2\n",
    "        cm.concatenate([movie_orig.resize(1, 1, ds_ratio) - mot_correct.min_mov*mot_correct.nonneg_movie,\n",
    "                        movie_corrected.resize(1, 1, ds_ratio)], \n",
    "                        axis=2).play(fr=30, \n",
    "                                     gain=2, \n",
    "                                     magnification=3,\n",
    "                                     do_loop=False,\n",
    "                                     save_movie=True,\n",
    "                                     opencv_codec=save_codec,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "   \n",
    "If are working with a list of movies, the `subindices` filter can also be applied with `load_movie_chain()` and the filter will be applied to each movie in the list and `load_movie_chain()` will return the concatenated result.\n",
    "    \n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=caiman_estimators></a>\n",
    "## Appendix 4: Estimators and parameters\n",
    "\n",
    "For the main computations in the pipeline -- like motion correction and CNMF -- Caiman breaks things into two steps:\n",
    "\n",
    "- Construct estimator object (e.g., `MotionCorrect`, `CNMF`) by sending it the set of parameters it will use. \n",
    "- Run the method on the object to generate the results. For `CNMF` this will be the `fit()` method, for motion correction it is `motion_correct()`.\n",
    "\n",
    "This modular architecture, where models are initialized with parameters, and then estimates are made with a separate call to a method that carries out the calculations on data fed to the model, is useful for a few reasons. For one, it allows for  efficient exploration of parameter space. Often, after setting some *initial* set of parameters, you will want to modify the parameters after exploring your data (e.g., after viewing the size of the neurons, or looking at effects of changing correlation thresholds when running `CNMFE`). \n",
    "\n",
    "Note that our API is similar to the interface used by the [scikit-learn](https://scikit-learn.org/stable/) machine learning library. From their [manuscript on api design](https://arxiv.org/abs/1309.0238):\n",
    "\n",
    "    Estimator initialization and actual learning are strictly separated...The constructor of an estimator does not see any actual data, nor does it perform any actual learning. All it does is attach the given parameters to the object....Actual learning is performed by the `fit` method. p 4-5\n",
    "\n",
    "Thanks to Kushal Kolar for pointing out this document.\n",
    "\n",
    "TODO: add something about how to change parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.cluster.setup_cluster?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=the_cluster></a>\n",
    "## Appendix 5: More on cluster setup (and shutdown)\n",
    "Caiman is optimized for parallelization and works well at HPC centers as well as laptops with multiple CPU cores. The cluster is set up with the `setup_cluster()` function, which takes in multiple parameters:\n",
    "\n",
    "    c, multiprocessing_pool, n_processes = cm.cluster.setup_cluster(backend='multiprocessing', \n",
    "                                                     n_processes=None, \n",
    "                                                     ignore_preexisting=False)\n",
    "\n",
    "The `backend` parameter determines the type of cluster used. The default value, `'multiprocessing'`, uses the multiprocessing package, but `ipyparallel` is also available. More information on these choices can be found [here](https://github.com/flatironinstitute/CaImAn/blob/master/docs/CLUSTER.md). You can set the number of processes (cpu cores) to use with the `n_processes` variable: the default value `None` will lead to the function selecting one *less* than the total number of logical cores available.  \n",
    "\n",
    "The parameter `ignore_preexisting`, which defaults to `False`, is a failsafe used to avoid overwhelming your resources. If you try to start another cluster, you will get an error. However, sometimes on more powerful machines may want to spin up multiple Caiman environments. In that case, set `ignore_preexisting` to `True` and you will not get an error.\n",
    "\n",
    "As discussed in the main notebook, the output variable `multiprocessing_pool` returned by the function is the multicore processing object that will be used in subsequent processing steps. It is a crucial variable for Caiman's parallelization: it is the multicore engine that will be passed around in subsequent stages and is the fulcrum for parallelization. \n",
    "\n",
    "The other output that can be useful to check is `n_proceses`, as it will tell you how many processes you successfully set up. \n",
    "\n",
    "Once you are done running computations that will use the cluster (motion correction and CNMF), then it can be a useful to save CPU resources by shutting it down: \n",
    "\n",
    "    cm.stop_server(dview=multiprocessing_pool)\n",
    "    \n",
    "You may also have noticed that we used this method to shut down any pre-existing clusters before starting a new one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=memory_mapping></a>\n",
    "## Appendix 6: Memory mapping\n",
    "Not implemented yet. Maybe don't ever implement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
